{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import *\n",
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dkleczek_bert_model = BertForMaskedLM.from_pretrained(\"dkleczek/bert-base-polish-cased-v1\")\n",
    "dkleczek_bert_tokenizer = BertTokenizer.from_pretrained(\"dkleczek/bert-base-polish-cased-v1\")\n",
    "\n",
    "allegro_base_model = BertForMaskedLM.from_pretrained(\"allegro/herbert-base-cased\")\n",
    "allegro_base_tokenizer = AutoTokenizer.from_pretrained(\"allegro/herbert-base-cased\")\n",
    "\n",
    "allegro_large_model = BertForMaskedLM.from_pretrained(\"allegro/herbert-large-cased\")\n",
    "allegro_large_tokenizer = AutoTokenizer.from_pretrained(\"allegro/herbert-large-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dkleczek_bert_predict(sentence):\n",
    "    nlp = pipeline('fill-mask', model=dkleczek_bert_model, tokenizer=dkleczek_bert_tokenizer)\n",
    "    sentence = sentence.replace('<mask>', f'{nlp.tokenizer.mask_token}')\n",
    "    predictions = list(map(lambda p: p['token_str'], nlp(sentence)))\n",
    "    return predictions\n",
    "\n",
    "def allegro_base_predict(sentence):\n",
    "    nlp = pipeline('fill-mask', model=allegro_base_model, tokenizer=allegro_base_tokenizer)\n",
    "    predictions = list(map(lambda p: p['token_str'], nlp(sentence)))\n",
    "    return predictions\n",
    "\n",
    "def allegro_large_predict(sentence):\n",
    "    nlp = pipeline('fill-mask', model=allegro_large_model, tokenizer=allegro_large_tokenizer)\n",
    "    predictions = list(map(lambda p: p['token_str'], nlp(sentence)))\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Produce the predictions for the following sentences (use each model and check 5 predictions):\n",
    "* (M) Warszawa to największe [MASK].\n",
    "* (D) Te zabawki należą do [MASK].\n",
    "* (C) Policjant przygląda się [MASK].\n",
    "* (B) Na środku skrzyżowania widać [MASK].\n",
    "* (N) Właściciel samochodu widział złodzieja z [MASK].\n",
    "* (Ms) Prezydent z premierem rozmawiali wczoraj o [MASK].\n",
    "* (W) Witaj drogi [MASK]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    f'Warszawa to największe <mask>',\n",
    "    f'Te zabawki należą do <mask>',\n",
    "    f'Policjant przygląda się <mask>',\n",
    "    f'Na środku skrzyżowania widać <mask>',\n",
    "    f'Właściciel samochodu widział złodzieja z <mask>',\n",
    "    f'Prezydent z premierem rozmawiali wczoraj o <mask>',\n",
    "    f'Witaj drogi <mask>'\n",
    "]\n",
    "\n",
    "def print_prediction(predictions, model_name, sentence):\n",
    "    print(f'\\tUsing: {model_name}')\n",
    "    for predicted_word in predictions:\n",
    "        prediction = sentence.replace('<mask>', colored(predicted_word, 'blue'))\n",
    "        print(f'\\t\\t{prediction}')\n",
    "    print('\\n')\n",
    "    \n",
    "def print_predictions_for(sentences):\n",
    "    for sentence in sentences:\n",
    "        print(f'Prediction for: {sentence}')\n",
    "        print_prediction(dkleczek_bert_predict(sentence)[:5], 'dkleczek/bert-base-polish-cased-v1', sentence)\n",
    "        print_prediction(allegro_base_predict(sentence)[:5], 'allegro/herbert-base-cased', sentence)\n",
    "        print_prediction(allegro_large_predict(sentence)[:5], 'allegro/herbert-large-cased', sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for: Warszawa to największe <mask>\n",
      "\tUsing: dkleczek/bert-base-polish-cased-v1\n",
      "\t\tWarszawa to największe \u001b[34mmiasto\u001b[0m\n",
      "\t\tWarszawa to największe \u001b[34m.\u001b[0m\n",
      "\t\tWarszawa to największe \u001b[34mmiejsce\u001b[0m\n",
      "\t\tWarszawa to największe \u001b[34mwojewództwo\u001b[0m\n",
      "\t\tWarszawa to największe \u001b[34mlotnisko\u001b[0m\n",
      "\n",
      "\n",
      "\tUsing: allegro/herbert-base-cased\n",
      "\t\tWarszawa to największe \u001b[34mmiasto</w>\u001b[0m\n",
      "\t\tWarszawa to największe \u001b[34m…</w>\u001b[0m\n",
      "\t\tWarszawa to największe \u001b[34mmiasta</w>\u001b[0m\n",
      "\t\tWarszawa to największe \u001b[34mlotnisko</w>\u001b[0m\n",
      "\t\tWarszawa to największe \u001b[34mcentrum</w>\u001b[0m\n",
      "\n",
      "\n",
      "\tUsing: allegro/herbert-large-cased\n",
      "\t\tWarszawa to największe \u001b[34mmiasto</w>\u001b[0m\n",
      "\t\tWarszawa to największe \u001b[34m…</w>\u001b[0m\n",
      "\t\tWarszawa to największe \u001b[34mcentrum</w>\u001b[0m\n",
      "\t\tWarszawa to największe \u001b[34mmiasta</w>\u001b[0m\n",
      "\t\tWarszawa to największe \u001b[34mlotnisko</w>\u001b[0m\n",
      "\n",
      "\n",
      "Prediction for: Te zabawki należą do <mask>\n",
      "\tUsing: dkleczek/bert-base-polish-cased-v1\n",
      "\t\tTe zabawki należą do \u001b[34mciebie\u001b[0m\n",
      "\t\tTe zabawki należą do \u001b[34mdzieci\u001b[0m\n",
      "\t\tTe zabawki należą do \u001b[34mmnie\u001b[0m\n",
      "\t\tTe zabawki należą do \u001b[34mCiebie\u001b[0m\n",
      "\t\tTe zabawki należą do \u001b[34mpana\u001b[0m\n",
      "\n",
      "\n",
      "\tUsing: allegro/herbert-base-cased\n",
      "\t\tTe zabawki należą do \u001b[34m…</w>\u001b[0m\n",
      "\t\tTe zabawki należą do \u001b[34mrodziny</w>\u001b[0m\n",
      "\t\tTe zabawki należą do \u001b[34mnajlepszych</w>\u001b[0m\n",
      "\t\tTe zabawki należą do \u001b[34m:</w>\u001b[0m\n",
      "\t\tTe zabawki należą do \u001b[34mnich</w>\u001b[0m\n",
      "\n",
      "\n",
      "\tUsing: allegro/herbert-large-cased\n",
      "\t\tTe zabawki należą do \u001b[34m…</w>\u001b[0m\n",
      "\t\tTe zabawki należą do \u001b[34mnas</w>\u001b[0m\n",
      "\t\tTe zabawki należą do \u001b[34mmnie</w>\u001b[0m\n",
      "\t\tTe zabawki należą do \u001b[34mprzeszłości</w>\u001b[0m\n",
      "\t\tTe zabawki należą do \u001b[34mnich</w>\u001b[0m\n",
      "\n",
      "\n",
      "Prediction for: Policjant przygląda się <mask>\n",
      "\tUsing: dkleczek/bert-base-polish-cased-v1\n",
      "\t\tPolicjant przygląda się \u001b[34m.\u001b[0m\n",
      "\t\tPolicjant przygląda się \u001b[34m!\u001b[0m\n",
      "\t\tPolicjant przygląda się \u001b[34m?\u001b[0m\n",
      "\t\tPolicjant przygląda się \u001b[34mpolicji\u001b[0m\n",
      "\t\tPolicjant przygląda się \u001b[34mtemu\u001b[0m\n",
      "\n",
      "\n",
      "\tUsing: allegro/herbert-base-cased\n",
      "\t\tPolicjant przygląda się \u001b[34mpolicji</w>\u001b[0m\n",
      "\t\tPolicjant przygląda się \u001b[34m…</w>\u001b[0m\n",
      "\t\tPolicjant przygląda się \u001b[34mkobiecie</w>\u001b[0m\n",
      "\t\tPolicjant przygląda się \u001b[34mmężczyźnie</w>\u001b[0m\n",
      "\t\tPolicjant przygląda się \u001b[34mpolicjantom</w>\u001b[0m\n",
      "\n",
      "\n",
      "\tUsing: allegro/herbert-large-cased\n",
      "\t\tPolicjant przygląda się \u001b[34m…</w>\u001b[0m\n",
      "\t\tPolicjant przygląda się \u001b[34m.</w>\u001b[0m\n",
      "\t\tPolicjant przygląda się \u001b[34msprawie</w>\u001b[0m\n",
      "\t\tPolicjant przygląda się \u001b[34mjej</w>\u001b[0m\n",
      "\t\tPolicjant przygląda się \u001b[34mtemu</w>\u001b[0m\n",
      "\n",
      "\n",
      "Prediction for: Na środku skrzyżowania widać <mask>\n",
      "\tUsing: dkleczek/bert-base-polish-cased-v1\n",
      "\t\tNa środku skrzyżowania widać \u001b[34m:\u001b[0m\n",
      "\t\tNa środku skrzyżowania widać \u001b[34m.\u001b[0m\n",
      "\t\tNa środku skrzyżowania widać \u001b[34mnapis\u001b[0m\n",
      "\t\tNa środku skrzyżowania widać \u001b[34m,\u001b[0m\n",
      "\t\tNa środku skrzyżowania widać \u001b[34mulicę\u001b[0m\n",
      "\n",
      "\n",
      "\tUsing: allegro/herbert-base-cased\n",
      "\t\tNa środku skrzyżowania widać \u001b[34m…</w>\u001b[0m\n",
      "\t\tNa środku skrzyżowania widać \u001b[34m:</w>\u001b[0m\n",
      "\t\tNa środku skrzyżowania widać \u001b[34m.</w>\u001b[0m\n",
      "\t\tNa środku skrzyżowania widać \u001b[34msamochód</w>\u001b[0m\n",
      "\t\tNa środku skrzyżowania widać \u001b[34mskrzyżowanie</w>\u001b[0m\n",
      "\n",
      "\n",
      "\tUsing: allegro/herbert-large-cased\n",
      "\t\tNa środku skrzyżowania widać \u001b[34m…</w>\u001b[0m\n",
      "\t\tNa środku skrzyżowania widać \u001b[34m:</w>\u001b[0m\n",
      "\t\tNa środku skrzyżowania widać \u001b[34mskrzyżowanie</w>\u001b[0m\n",
      "\t\tNa środku skrzyżowania widać \u001b[34m.</w>\u001b[0m\n",
      "\t\tNa środku skrzyżowania widać \u001b[34mrondo</w>\u001b[0m\n",
      "\n",
      "\n",
      "Prediction for: Właściciel samochodu widział złodzieja z <mask>\n",
      "\tUsing: dkleczek/bert-base-polish-cased-v1\n",
      "\t\tWłaściciel samochodu widział złodzieja z \u001b[34mtyłu\u001b[0m\n",
      "\t\tWłaściciel samochodu widział złodzieja z \u001b[34msamochodu\u001b[0m\n",
      "\t\tWłaściciel samochodu widział złodzieja z \u001b[34mbronią\u001b[0m\n",
      "\t\tWłaściciel samochodu widział złodzieja z \u001b[34mulicy\u001b[0m\n",
      "\t\tWłaściciel samochodu widział złodzieja z \u001b[34mbliska\u001b[0m\n",
      "\n",
      "\n",
      "\tUsing: allegro/herbert-base-cased\n",
      "\t\tWłaściciel samochodu widział złodzieja z \u001b[34mwłamaniem</w>\u001b[0m\n",
      "\t\tWłaściciel samochodu widział złodzieja z \u001b[34msamochodu</w>\u001b[0m\n",
      "\t\tWłaściciel samochodu widział złodzieja z \u001b[34mparkingu</w>\u001b[0m\n",
      "\t\tWłaściciel samochodu widział złodzieja z \u001b[34mbronią</w>\u001b[0m\n",
      "\t\tWłaściciel samochodu widział złodzieja z \u001b[34mpolicji</w>\u001b[0m\n",
      "\n",
      "\n",
      "\tUsing: allegro/herbert-large-cased\n",
      "\t\tWłaściciel samochodu widział złodzieja z \u001b[34mBMW</w>\u001b[0m\n",
      "\t\tWłaściciel samochodu widział złodzieja z \u001b[34m…</w>\u001b[0m\n",
      "\t\tWłaściciel samochodu widział złodzieja z \u001b[34mwłamaniem</w>\u001b[0m\n",
      "\t\tWłaściciel samochodu widział złodzieja z \u001b[34msamochodu</w>\u001b[0m\n",
      "\t\tWłaściciel samochodu widział złodzieja z \u001b[34mkradzieży</w>\u001b[0m\n",
      "\n",
      "\n",
      "Prediction for: Prezydent z premierem rozmawiali wczoraj o <mask>\n",
      "\tUsing: dkleczek/bert-base-polish-cased-v1\n",
      "\t\tPrezydent z premierem rozmawiali wczoraj o \u001b[34mtym\u001b[0m\n",
      "\t\tPrezydent z premierem rozmawiali wczoraj o \u001b[34m:\u001b[0m\n",
      "\t\tPrezydent z premierem rozmawiali wczoraj o \u001b[34m…\u001b[0m\n",
      "\t\tPrezydent z premierem rozmawiali wczoraj o \u001b[34mprzyszłości\u001b[0m\n",
      "\t\tPrezydent z premierem rozmawiali wczoraj o \u001b[34m.\u001b[0m\n",
      "\n",
      "\n",
      "\tUsing: allegro/herbert-base-cased\n",
      "\t\tPrezydent z premierem rozmawiali wczoraj o \u001b[34m…</w>\u001b[0m\n",
      "\t\tPrezydent z premierem rozmawiali wczoraj o \u001b[34mpolityce</w>\u001b[0m\n",
      "\t\tPrezydent z premierem rozmawiali wczoraj o \u001b[34mbezpieczeństwie</w>\u001b[0m\n",
      "\t\tPrezydent z premierem rozmawiali wczoraj o \u001b[34mPolsce</w>\u001b[0m\n",
      "\t\tPrezydent z premierem rozmawiali wczoraj o \u001b[34mprojekcie</w>\u001b[0m\n",
      "\n",
      "\n",
      "\tUsing: allegro/herbert-large-cased\n",
      "\t\tPrezydent z premierem rozmawiali wczoraj o \u001b[34mIraku</w>\u001b[0m\n",
      "\t\tPrezydent z premierem rozmawiali wczoraj o \u001b[34mUkrainie</w>\u001b[0m\n",
      "\t\tPrezydent z premierem rozmawiali wczoraj o \u001b[34mkryzysie</w>\u001b[0m\n",
      "\t\tPrezydent z premierem rozmawiali wczoraj o \u001b[34mreferendum</w>\u001b[0m\n",
      "\t\tPrezydent z premierem rozmawiali wczoraj o \u001b[34maborcji</w>\u001b[0m\n",
      "\n",
      "\n",
      "Prediction for: Witaj drogi <mask>\n",
      "\tUsing: dkleczek/bert-base-polish-cased-v1\n",
      "\t\tWitaj drogi \u001b[34m.\u001b[0m\n",
      "\t\tWitaj drogi \u001b[34m!\u001b[0m\n",
      "\t\tWitaj drogi \u001b[34m?\u001b[0m\n",
      "\t\tWitaj drogi \u001b[34m,\u001b[0m\n",
      "\t\tWitaj drogi \u001b[34mprzyjacielu\u001b[0m\n",
      "\n",
      "\n",
      "\tUsing: allegro/herbert-base-cased\n",
      "\t\tWitaj drogi \u001b[34m!</w>\u001b[0m\n",
      "\t\tWitaj drogi \u001b[34m.</w>\u001b[0m\n",
      "\t\tWitaj drogi \u001b[34m,</w>\u001b[0m\n",
      "\t\tWitaj drogi \u001b[34mBoże</w>\u001b[0m\n",
      "\t\tWitaj drogi \u001b[34m…</w>\u001b[0m\n",
      "\n",
      "\n",
      "\tUsing: allegro/herbert-large-cased\n",
      "\t\tWitaj drogi \u001b[34m!</w>\u001b[0m\n",
      "\t\tWitaj drogi \u001b[34m.</w>\u001b[0m\n",
      "\t\tWitaj drogi \u001b[34m…</w>\u001b[0m\n",
      "\t\tWitaj drogi \u001b[34m,</w>\u001b[0m\n",
      "\t\tWitaj drogi \u001b[34mczłowieku</w>\u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_predictions_for(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the model predictions for the following sentences (using each model):\n",
    "* Gdybym wiedział wtedy dokładnie to co wiem teraz, to bym się nie [MASK].\n",
    "* Gdybym wiedziała wtedy dokładnie to co wiem teraz, to bym się nie [MASK]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences2 = [\n",
    "    'Gdybym wiedział wtedy dokładnie to co wiem teraz, to bym się nie <mask>',\n",
    "    'Gdybym wiedziała wtedy dokładnie to co wiem teraz, to bym się nie <mask>'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for: Gdybym wiedział wtedy dokładnie to co wiem teraz, to bym się nie <mask>\n",
      "\tUsing: dkleczek/bert-base-polish-cased-v1\n",
      "\t\tGdybym wiedział wtedy dokładnie to co wiem teraz, to bym się nie \u001b[34mbał\u001b[0m\n",
      "\t\tGdybym wiedział wtedy dokładnie to co wiem teraz, to bym się nie \u001b[34mprzyznał\u001b[0m\n",
      "\t\tGdybym wiedział wtedy dokładnie to co wiem teraz, to bym się nie \u001b[34mzgodził\u001b[0m\n",
      "\t\tGdybym wiedział wtedy dokładnie to co wiem teraz, to bym się nie \u001b[34mśmiał\u001b[0m\n",
      "\t\tGdybym wiedział wtedy dokładnie to co wiem teraz, to bym się nie \u001b[34mmartwił\u001b[0m\n",
      "\n",
      "\n",
      "\tUsing: allegro/herbert-base-cased\n",
      "\t\tGdybym wiedział wtedy dokładnie to co wiem teraz, to bym się nie \u001b[34mdowiedział</w>\u001b[0m\n",
      "\t\tGdybym wiedział wtedy dokładnie to co wiem teraz, to bym się nie \u001b[34mzdziwił</w>\u001b[0m\n",
      "\t\tGdybym wiedział wtedy dokładnie to co wiem teraz, to bym się nie \u001b[34mpoddał</w>\u001b[0m\n",
      "\t\tGdybym wiedział wtedy dokładnie to co wiem teraz, to bym się nie \u001b[34mzastanawiał</w>\u001b[0m\n",
      "\t\tGdybym wiedział wtedy dokładnie to co wiem teraz, to bym się nie \u001b[34mzorientował</w>\u001b[0m\n",
      "\n",
      "\n",
      "\tUsing: allegro/herbert-large-cased\n",
      "\t\tGdybym wiedział wtedy dokładnie to co wiem teraz, to bym się nie \u001b[34możenił</w>\u001b[0m\n",
      "\t\tGdybym wiedział wtedy dokładnie to co wiem teraz, to bym się nie \u001b[34mbał</w>\u001b[0m\n",
      "\t\tGdybym wiedział wtedy dokładnie to co wiem teraz, to bym się nie \u001b[34mzabił</w>\u001b[0m\n",
      "\t\tGdybym wiedział wtedy dokładnie to co wiem teraz, to bym się nie \u001b[34mśmiał</w>\u001b[0m\n",
      "\t\tGdybym wiedział wtedy dokładnie to co wiem teraz, to bym się nie \u001b[34mpoddał</w>\u001b[0m\n",
      "\n",
      "\n",
      "Prediction for: Gdybym wiedziała wtedy dokładnie to co wiem teraz, to bym się nie <mask>\n",
      "\tUsing: dkleczek/bert-base-polish-cased-v1\n",
      "\t\tGdybym wiedziała wtedy dokładnie to co wiem teraz, to bym się nie \u001b[34mzgodziła\u001b[0m\n",
      "\t\tGdybym wiedziała wtedy dokładnie to co wiem teraz, to bym się nie \u001b[34mbała\u001b[0m\n",
      "\t\tGdybym wiedziała wtedy dokładnie to co wiem teraz, to bym się nie \u001b[34mdowiedziała\u001b[0m\n",
      "\t\tGdybym wiedziała wtedy dokładnie to co wiem teraz, to bym się nie \u001b[34mpojawiła\u001b[0m\n",
      "\t\tGdybym wiedziała wtedy dokładnie to co wiem teraz, to bym się nie \u001b[34mzabiła\u001b[0m\n",
      "\n",
      "\n",
      "\tUsing: allegro/herbert-base-cased\n",
      "\t\tGdybym wiedziała wtedy dokładnie to co wiem teraz, to bym się nie \u001b[34mdowiedziała</w>\u001b[0m\n",
      "\t\tGdybym wiedziała wtedy dokładnie to co wiem teraz, to bym się nie \u001b[34mbała</w>\u001b[0m\n",
      "\t\tGdybym wiedziała wtedy dokładnie to co wiem teraz, to bym się nie \u001b[34mprzyznała</w>\u001b[0m\n",
      "\t\tGdybym wiedziała wtedy dokładnie to co wiem teraz, to bym się nie \u001b[34mzmieniła</w>\u001b[0m\n",
      "\t\tGdybym wiedziała wtedy dokładnie to co wiem teraz, to bym się nie \u001b[34mśmiała</w>\u001b[0m\n",
      "\n",
      "\n",
      "\tUsing: allegro/herbert-large-cased\n",
      "\t\tGdybym wiedziała wtedy dokładnie to co wiem teraz, to bym się nie \u001b[34mbała</w>\u001b[0m\n",
      "\t\tGdybym wiedziała wtedy dokładnie to co wiem teraz, to bym się nie \u001b[34mzgodziła</w>\u001b[0m\n",
      "\t\tGdybym wiedziała wtedy dokładnie to co wiem teraz, to bym się nie \u001b[34mdowiedziała</w>\u001b[0m\n",
      "\t\tGdybym wiedziała wtedy dokładnie to co wiem teraz, to bym się nie \u001b[34murodziła</w>\u001b[0m\n",
      "\t\tGdybym wiedziała wtedy dokładnie to co wiem teraz, to bym się nie \u001b[34mśmiała</w>\u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_predictions_for(sentences2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the model predictions for the following sentences:\n",
    "* [MASK] wrze w temperaturze 100 stopni, a zamarza w temeraturze 0 stopni Celsjusza.\n",
    "* W wakacje odwiedziłem [MASK], który jest stolicą Islandii.\n",
    "* Informatyka na [MASK] należy do najlepszych kierunków w Polsce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences3 = [\n",
    "    '<mask> wrze w temperaturze 100 stopni, a zamarza w temeraturze 0 stopni Celsjusza.',\n",
    "    'W wakacje odwiedziłem <mask>, który jest stolicą Islandii.',\n",
    "    'Informatyka na <mask> należy do najlepszych kierunków w Polsce.'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for: <mask> wrze w temperaturze 100 stopni, a zamarza w temeraturze 0 stopni Celsjusza.\n",
      "\tUsing: dkleczek/bert-base-polish-cased-v1\n",
      "\t\t\u001b[34mWoda\u001b[0m wrze w temperaturze 100 stopni, a zamarza w temeraturze 0 stopni Celsjusza.\n",
      "\t\t\u001b[34mMięso\u001b[0m wrze w temperaturze 100 stopni, a zamarza w temeraturze 0 stopni Celsjusza.\n",
      "\t\t\u001b[34mSłońce\u001b[0m wrze w temperaturze 100 stopni, a zamarza w temeraturze 0 stopni Celsjusza.\n",
      "\t\t\u001b[34mNie\u001b[0m wrze w temperaturze 100 stopni, a zamarza w temeraturze 0 stopni Celsjusza.\n",
      "\t\t\u001b[34mPiwo\u001b[0m wrze w temperaturze 100 stopni, a zamarza w temeraturze 0 stopni Celsjusza.\n",
      "\n",
      "\n",
      "\tUsing: allegro/herbert-base-cased\n",
      "\t\t\u001b[34mWoda</w>\u001b[0m wrze w temperaturze 100 stopni, a zamarza w temeraturze 0 stopni Celsjusza.\n",
      "\t\t\u001b[34mSłońce</w>\u001b[0m wrze w temperaturze 100 stopni, a zamarza w temeraturze 0 stopni Celsjusza.\n",
      "\t\t\u001b[34mZiemia</w>\u001b[0m wrze w temperaturze 100 stopni, a zamarza w temeraturze 0 stopni Celsjusza.\n",
      "\t\t\u001b[34mNastępnie</w>\u001b[0m wrze w temperaturze 100 stopni, a zamarza w temeraturze 0 stopni Celsjusza.\n",
      "\t\t\u001b[34mCiało</w>\u001b[0m wrze w temperaturze 100 stopni, a zamarza w temeraturze 0 stopni Celsjusza.\n",
      "\n",
      "\n",
      "\tUsing: allegro/herbert-large-cased\n",
      "\t\t\u001b[34mWoda</w>\u001b[0m wrze w temperaturze 100 stopni, a zamarza w temeraturze 0 stopni Celsjusza.\n",
      "\t\t\u001b[34mKrew</w>\u001b[0m wrze w temperaturze 100 stopni, a zamarza w temeraturze 0 stopni Celsjusza.\n",
      "\t\t\u001b[34mwoda</w>\u001b[0m wrze w temperaturze 100 stopni, a zamarza w temeraturze 0 stopni Celsjusza.\n",
      "\t\t\u001b[34mOgień</w>\u001b[0m wrze w temperaturze 100 stopni, a zamarza w temeraturze 0 stopni Celsjusza.\n",
      "\t\t\u001b[34mNie</w>\u001b[0m wrze w temperaturze 100 stopni, a zamarza w temeraturze 0 stopni Celsjusza.\n",
      "\n",
      "\n",
      "Prediction for: W wakacje odwiedziłem <mask>, który jest stolicą Islandii.\n",
      "\tUsing: dkleczek/bert-base-polish-cased-v1\n",
      "\t\tW wakacje odwiedziłem \u001b[34mkraj\u001b[0m, który jest stolicą Islandii.\n",
      "\t\tW wakacje odwiedziłem \u001b[34mCypr\u001b[0m, który jest stolicą Islandii.\n",
      "\t\tW wakacje odwiedziłem \u001b[34mMeksyk\u001b[0m, który jest stolicą Islandii.\n",
      "\t\tW wakacje odwiedziłem \u001b[34mGibraltar\u001b[0m, który jest stolicą Islandii.\n",
      "\t\tW wakacje odwiedziłem \u001b[34mWellington\u001b[0m, który jest stolicą Islandii.\n",
      "\n",
      "\n",
      "\tUsing: allegro/herbert-base-cased\n",
      "\t\tW wakacje odwiedziłem \u001b[34mKraków</w>\u001b[0m, który jest stolicą Islandii.\n",
      "\t\tW wakacje odwiedziłem \u001b[34mOslo</w>\u001b[0m, który jest stolicą Islandii.\n",
      "\t\tW wakacje odwiedziłem \u001b[34mLondyn</w>\u001b[0m, który jest stolicą Islandii.\n",
      "\t\tW wakacje odwiedziłem \u001b[34mGdańsk</w>\u001b[0m, który jest stolicą Islandii.\n",
      "\t\tW wakacje odwiedziłem \u001b[34mToruń</w>\u001b[0m, który jest stolicą Islandii.\n",
      "\n",
      "\n",
      "\tUsing: allegro/herbert-large-cased\n",
      "\t\tW wakacje odwiedziłem \u001b[34mOslo</w>\u001b[0m, który jest stolicą Islandii.\n",
      "\t\tW wakacje odwiedziłem \u001b[34mLondyn</w>\u001b[0m, który jest stolicą Islandii.\n",
      "\t\tW wakacje odwiedziłem \u001b[34mLiverpool</w>\u001b[0m, który jest stolicą Islandii.\n",
      "\t\tW wakacje odwiedziłem \u001b[34mGlasgow</w>\u001b[0m, który jest stolicą Islandii.\n",
      "\t\tW wakacje odwiedziłem \u001b[34mBirmingham</w>\u001b[0m, który jest stolicą Islandii.\n",
      "\n",
      "\n",
      "Prediction for: Informatyka na <mask> należy do najlepszych kierunków w Polsce.\n",
      "\tUsing: dkleczek/bert-base-polish-cased-v1\n",
      "\t\tInformatyka na \u001b[34mwsi\u001b[0m należy do najlepszych kierunków w Polsce.\n",
      "\t\tInformatyka na \u001b[34mświecie\u001b[0m należy do najlepszych kierunków w Polsce.\n",
      "\t\tInformatyka na \u001b[34mżywo\u001b[0m należy do najlepszych kierunków w Polsce.\n",
      "\t\tInformatyka na \u001b[34mpewno\u001b[0m należy do najlepszych kierunków w Polsce.\n",
      "\t\tInformatyka na \u001b[34modległość\u001b[0m należy do najlepszych kierunków w Polsce.\n",
      "\n",
      "\n",
      "\tUsing: allegro/herbert-base-cased\n",
      "\t\tInformatyka na \u001b[34mpewno</w>\u001b[0m należy do najlepszych kierunków w Polsce.\n",
      "\t\tInformatyka na \u001b[34mAGH</w>\u001b[0m należy do najlepszych kierunków w Polsce.\n",
      "\t\tInformatyka na \u001b[34mUW</w>\u001b[0m należy do najlepszych kierunków w Polsce.\n",
      "\t\tInformatyka na \u001b[34mstudiach</w>\u001b[0m należy do najlepszych kierunków w Polsce.\n",
      "\t\tInformatyka na \u001b[34mUMK</w>\u001b[0m należy do najlepszych kierunków w Polsce.\n",
      "\n",
      "\n",
      "\tUsing: allegro/herbert-large-cased\n",
      "\t\tInformatyka na \u001b[34mAGH</w>\u001b[0m należy do najlepszych kierunków w Polsce.\n",
      "\t\tInformatyka na \u001b[34mUW</w>\u001b[0m należy do najlepszych kierunków w Polsce.\n",
      "\t\tInformatyka na \u001b[34mUJ</w>\u001b[0m należy do najlepszych kierunków w Polsce.\n",
      "\t\tInformatyka na \u001b[34mUAM</w>\u001b[0m należy do najlepszych kierunków w Polsce.\n",
      "\t\tInformatyka na \u001b[34muczelni</w>\u001b[0m należy do najlepszych kierunków w Polsce.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_predictions_for(sentences3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer the following questions:\n",
    "## Which of the models produced the best results?\n",
    "W wielu przypadkach _allegro_herbert_large_ oraz model _dkleczek_ dawały wyniki, które są równie prawdopodobne, ale np. dla \"Właściciel samochodu widział złodzieja z\" _dkleczek_ podpowiedział słowo \"tyłu\" jako jedyny, dla \"Policjant przygląda się __sprawie__\" to _allegro_large jako jedyne podpowiedziało naturalne słowo \"sprawie\". W innych przypadkach jeśli miałbym wskazać jeden to zastanowiłbym się który szybciej działa (_allegro_large_ czy _dkleczek_) gdyż oba poradziły sobie równie dobrze, być może z małym wskazaniem na _dkleczek_. Miał on tylko jedną duża wpadkę: \"Informatyka na wsi należy do najlepszych kierunków w Polsce.\" podczas gdy _allegro_herbert_large_ podpowiedziało nazwy uczelni (z AGH na pierwszym miejscu :D ). Jeżeli chodzi o wybrany przez mnie dla porównania model _allegro_ w wersji podstawowej (_allegro/herbert-base-cased_) to tak jak się spodziewałem dał on gorsze wyniki od swojej większej wersji, natomiast były one również zadowalające.\n",
    "\n",
    "## Was any of the models able to capture Polish grammar?\n",
    "Oceniając po drugiej grupie zdań (_sentences2_) gdzie należało dobrać rodzaj męski lub żeński to wszystkie modele poradziły sobie z tym zadaniem dobrze.\n",
    "\n",
    "## Was any of the models able to capture long-distant relationships between the words?\n",
    "Analogicznie do powyższego pytania jedyna różnica w grupie drugiej leżała 11 słów wcześniej, a wszystkie modele poradziły sobie dobrze.\n",
    "\n",
    "## Was any of the models able to capture world knowledge?\n",
    "Żadnemu modelowi nie udało się odgadnąć jaka jest stolica Islandii pomimo podpowiadania różnych miast. Jeżeli natomiast popatrzymy na zdanie w którym ewidętnie choidziło o wodę (\"\\<mask> wrze w temperaturze 100 stopni, a zamarza w temeraturze 0 stopni Celsjusza.\") to wszystkie modele sobie z nim poradziły i na pierwszym miejscu zwróciły właśnie słowo \"woda\". Ciężko więc jednoznacznie ocenińc jaką wiedzę są wstanie przenosić te modele, natomiast model _allegro_large_ był w stanie również w sensowny sposób podpowiedzieć \"Prezydent z premierem rozmawiali wczoraj o __Ukrainie__\" co sugeruje uczenie go nie tylko na książkach/wikipedii.\n",
    "\n",
    "## What are the most striking errors made by the models?\n",
    "Śmieszną odpowiedzią _allegro_large_ jest \"Właściciel samochodu widział złodzieja z __BMW__\" co sugeruję, że właśnie to jest marka złodzieji. Mało prawdopodobną odpowiedzią jest również \"Witaj drogi __człowieku__\" zaproponowane również przez _allegro_large_. Model _dkleczek_ zaproponował \"Prezydent z premierem rozmawiali wczoraj o __tym__\" co nie wnosi zbyt dużo do tego zdania."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
